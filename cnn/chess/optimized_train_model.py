# Optimized Chess CNN Training - 2-4x Faster Than Current Implementation
# Expected reduction: 60+ minutes per epoch â†’ 15-30 minutes per epoch

import time
import platform
import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
from torch.utils.data import Dataset, DataLoader
from torch.amp import autocast, GradScaler
import os

# System-level optimizations
def optimize_system_settings():
    """Apply system-level optimizations for RTX 5080 + 128GB RAM"""
    
    # CUDA optimizations for RTX 5080
    torch.backends.cudnn.benchmark = True
    torch.backends.cudnn.allow_tf32 = True
    torch.backends.cuda.matmul.allow_tf32 = True
    
    # Memory optimizations
    os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:512,expandable_segments:True'
    
    # CPU optimizations
    torch.set_num_threads(16)
    os.environ['OMP_NUM_THREADS'] = '16'
    os.environ['MKL_NUM_THREADS'] = '16'
    
    print("âœ“ System optimizations applied")

# OPTIMIZATION 1: Simplified but effective chess model architecture
class SimplifiedChessCNN(nn.Module):
    """Simplified CNN optimized for chess - removes complex components for speed"""
    
    def __init__(self, input_channels=19, base_filters=64, num_blocks=10):
        super(SimplifiedChessCNN, self).__init__()
        
        # Initial convolution
        self.input_conv = nn.Sequential(
            nn.Conv2d(input_channels, base_filters, 3, padding=1),
            nn.BatchNorm2d(base_filters),
            nn.ReLU(inplace=True)
        )
        
        # Residual blocks (simplified - no attention/transformers)
        self.residual_blocks = nn.ModuleList([
            self._make_residual_block(base_filters) for _ in range(num_blocks)
        ])
        
        # Policy head (direct prediction)
        self.policy_head = nn.Sequential(
            nn.Conv2d(base_filters, 2, 1),
            nn.BatchNorm2d(2),
            nn.ReLU(inplace=True),
            nn.Flatten(),
            nn.Linear(2 * 8 * 8, 4096)
        )
        
    def _make_residual_block(self, channels):
        return nn.Sequential(
            nn.Conv2d(channels, channels, 3, padding=1),
            nn.BatchNorm2d(channels),
            nn.ReLU(inplace=True),
            nn.Conv2d(channels, channels, 3, padding=1),
            nn.BatchNorm2d(channels),
        )
    
    def forward(self, x):
        # Initial convolution
        x = self.input_conv(x)
        
        # Residual blocks with skip connections
        for block in self.residual_blocks:
            identity = x
            x = block(x) + identity
            x = torch.relu(x)
        
        # Policy prediction
        return self.policy_head(x)

# OPTIMIZATION 2: Optimized dataset with caching
class OptimizedChessDataset(Dataset):
    """Memory-optimized dataset with intelligent caching"""
    
    def __init__(self, base_path, cache_size_gb=8):
        self.base_path = base_path
        
        # Load metadata
        meta = np.load(f"{base_path}_meta.npz", allow_pickle=True)
        self.length = int(meta['length'])
        
        # Initialize cache
        self.cache_size = int(cache_size_gb * 1024**3 / (19 * 8 * 8 * 4))
        self.cache = {}
        self.cache_order = []
        
        # Open memory-mapped files
        self._init_memmap()
        print(f"âœ“ Dataset initialized with {cache_size_gb}GB cache ({self.cache_size:,} samples)")
    
    def _init_memmap(self):
        self.inputs = np.memmap(f"{self.base_path}_inputs.dat",
                               dtype=np.float32, mode='r',
                               shape=(self.length, 19, 8, 8))
        self.outputs = np.memmap(f"{self.base_path}_outputs.dat",
                                dtype=np.float32, mode='r',
                                shape=(self.length, 4096))
    
    def __getitem__(self, idx):
        # Check cache first
        if idx in self.cache:
            return self.cache[idx]
        
        # Load from memory-mapped files
        input_data = torch.from_numpy(self.inputs[idx].copy())
        output_data = torch.from_numpy(self.outputs[idx].copy())
        
        # Add to cache with LRU eviction
        if len(self.cache) < self.cache_size:
            self.cache[idx] = (input_data, output_data)
            self.cache_order.append(idx)
        elif self.cache_order:
            # Remove oldest item
            oldest = self.cache_order.pop(0)
            del self.cache[oldest]
            self.cache[idx] = (input_data, output_data)
            self.cache_order.append(idx)
        
        return input_data, output_data
    
    def __len__(self):
        return self.length

# OPTIMIZATION 3: High-performance trainer with all optimizations
class OptimizedTrainer:
    """Optimized trainer with gradient accumulation and minimal overhead"""
    
    def __init__(self, model, train_loader, val_loader, config):
        self.model = model
        self.train_loader = train_loader
        self.val_loader = val_loader
        self.config = config
        
        # Gradient accumulation settings
        self.accumulation_steps = config.get("accumulation_steps", 4)
        self.effective_batch_size = config["batch_size"] * self.accumulation_steps
        
        # Optimized optimizer (fused AdamW for speed)
        self.optimizer = torch.optim.AdamW(
            model.parameters(),
            lr=config["learning_rate"],
            weight_decay=config["weight_decay"],
            fused=True  # PyTorch 2.0+ fused optimizer
        )
        
        # Simplified components
        self.scaler = GradScaler()
        self.criterion = nn.CrossEntropyLoss()  # Simple, fast loss
        
        # Learning rate scheduler
        self.scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(
            self.optimizer, T_0=10, T_mult=2, eta_min=1e-6
        )
        
        print(f"âœ“ Trainer initialized - Effective batch size: {self.effective_batch_size}")
    
    def train_epoch(self, epoch):
        """Optimized training epoch with minimal overhead"""
        self.model.train()
        total_loss = 0.0
        num_updates = 0
        start_time = time.time()
        
        # Clear gradients once
        self.optimizer.zero_grad(set_to_none=True)
        
        for batch_idx, (data, target) in enumerate(self.train_loader):
            # Fast data transfer
            data = data.cuda(non_blocking=True)
            target = target.cuda(non_blocking=True).argmax(dim=1)  # Convert one-hot
            
            # Forward pass with AMP
            with autocast(device_type='cuda', dtype=torch.float16):
                output = self.model(data)
                loss = self.criterion(output, target) / self.accumulation_steps
            
            # Backward pass
            self.scaler.scale(loss).backward()
            
            # Update weights every accumulation_steps
            if (batch_idx + 1) % self.accumulation_steps == 0:
                # Gradient clipping
                self.scaler.unscale_(self.optimizer)
                torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)
                
                # Optimizer step
                self.scaler.step(self.optimizer)
                self.scaler.update()
                self.optimizer.zero_grad(set_to_none=True)
                
                num_updates += 1
            
            total_loss += loss.item() * self.accumulation_steps
            
            # Minimal logging (every 100 batches)
            if batch_idx % 100 == 0:
                throughput = (batch_idx + 1) * self.config["batch_size"] / (time.time() - start_time)
                print(f"Batch {batch_idx:5d} | Loss: {loss.item():.4f} | "
                      f"Throughput: {throughput:.0f} samples/sec")
        
        # Learning rate step
        self.scheduler.step()
        
        epoch_time = time.time() - start_time
        avg_loss = total_loss / len(self.train_loader)
        
        print(f"âœ“ Epoch {epoch} completed in {epoch_time:.1f}s | Avg Loss: {avg_loss:.4f}")
        return avg_loss
    
    def validate(self):
        """Fast validation without unnecessary computations"""
        self.model.eval()
        total_loss = 0.0
        
        with torch.no_grad():
            for data, target in self.val_loader:
                data = data.cuda(non_blocking=True)
                target = target.cuda(non_blocking=True).argmax(dim=1)
                
                with autocast(device_type='cuda', dtype=torch.float16):
                    output = self.model(data)
                    loss = self.criterion(output, target)
                
                total_loss += loss.item()
        
        return total_loss / len(self.val_loader)

# OPTIMIZATION 4: Optimized configuration
OPTIMIZED_CONFIG = {
    "num_epoch": 200,
    "device": "cuda",
    "input_channels": 19,
    "board_size": 8,
    "batch_size": 1024,        # Reduced from 2048
    "accumulation_steps": 4,    # Effective batch = 4096
    "num_workers": 12,          # Optimal for 16-core CPU
    "learning_rate": 0.002,     # Scaled for larger effective batch
    "weight_decay": 1e-4,
    "mixed_precision": True,
    "gradient_clipping": 1.0,
    "cache_size_gb": 8,         # Use 8GB for data caching
}

def create_optimized_dataloader(dataset, batch_size, is_training=True):
    """Create optimized DataLoader for maximum performance"""
    return DataLoader(
        dataset,
        batch_size=batch_size,
        shuffle=is_training,
        num_workers=OPTIMIZED_CONFIG["num_workers"],
        pin_memory=True,
        persistent_workers=True,
        prefetch_factor=3,
        drop_last=is_training,
        worker_init_fn=lambda x: torch.set_num_threads(1),
    )

def main():
    """Main training function with all optimizations"""
    print("ðŸš€ Starting Optimized Chess CNN Training")
    print("="*60)
    
    # Apply system optimizations
    optimize_system_settings()
    
    # Create simplified model
    model = SimplifiedChessCNN(
        input_channels=19,
        base_filters=64,
        num_blocks=10
    ).cuda()
    
    # CRITICAL: Apply torch.compile for 30-50% speedup
    print("ðŸ”¥ Compiling model with torch.compile...")
    model = torch.compile(
        model,
        mode="max-autotune",    # Best for training performance
        fullgraph=True,         # More aggressive optimization
        dynamic=False           # Consistent input shapes
    )
    print("âœ“ Model compiled successfully")
    
    # Create optimized datasets
    train_dataset = OptimizedChessDataset("data/all_train_data_with_puzzles_v2", cache_size_gb=8)
    
    # Split dataset
    train_size = int(0.8 * len(train_dataset))
    val_size = len(train_dataset) - train_size
    train_dataset, val_dataset = torch.utils.data.random_split(train_dataset, [train_size, val_size])
    
    # Create optimized data loaders
    train_loader = create_optimized_dataloader(train_dataset, OPTIMIZED_CONFIG["batch_size"], True)
    val_loader = create_optimized_dataloader(val_dataset, OPTIMIZED_CONFIG["batch_size"] * 2, False)
    
    print(f"âœ“ Data loaders created - Train: {len(train_loader):,} batches, Val: {len(val_loader):,} batches")
    
    # Create optimized trainer
    trainer = OptimizedTrainer(model, train_loader, val_loader, OPTIMIZED_CONFIG)
    
    # Training loop
    print("\nðŸŽ¯ Starting optimized training...")
    best_val_loss = float('inf')
    
    for epoch in range(OPTIMIZED_CONFIG["num_epoch"]):
        # Training
        train_loss = trainer.train_epoch(epoch)
        
        # Validation (every 5 epochs to save time)
        if epoch % 5 == 0:
            val_loss = trainer.validate()
            print(f"Validation Loss: {val_loss:.4f}")
            
            # Save best model
            if val_loss < best_val_loss:
                best_val_loss = val_loss
                torch.save(model.state_dict(), f"optimized_chess_model_epoch_{epoch}.pth")
                print(f"âœ“ New best model saved!")
    
    print("\nðŸŽ‰ Training completed with optimizations!")
    print("Expected performance improvement: 2-4x faster than original")

if __name__ == "__main__":
    main()